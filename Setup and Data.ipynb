{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51da16d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\troen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = [\"acled1.csv\", \"acled2.csv\", \"acled3.csv\", \"acled4.csv\", \"acled5.csv\"]\n",
    "#dataframes = []\n",
    "\n",
    "# Reading and processing each dataset\n",
    "#for dataset in datasets:\n",
    " #   df = pd.read_csv(\"Data/\" + dataset)\n",
    "\n",
    "    # Columns to drop\n",
    "  #  columns_to_drop = ['notes', 'source', 'source_scale', 'geo_precision', \n",
    "                       'latitude', 'longitude', 'tags', 'event_date', \n",
    "                       'civilian_targeting', 'location', 'region', 'timestamp',\n",
    "                       'disorder_type', 'sub_event_type', 'admin1', 'admin2',\n",
    "                       'admin3', 'actor1', 'assoc_actor_1', 'time_precision', \n",
    "                       'inter1', 'interaction', 'fatalities']\n",
    "\n",
    "   # df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Dropping duplicates in column event_id_cnty (because only actor change)\n",
    "    #df = df.drop_duplicates(subset=['event_id_cnty'], keep='first')\n",
    "\n",
    "    # Counting the number of occurrences of each event_type per country per year\n",
    "    #pivot_table = df.pivot_table(index=['year', 'iso', 'country'], columns='event_type', aggfunc='size', fill_value=0)\n",
    "    #pivot_table.reset_index(inplace=True)\n",
    "\n",
    "    #dataframes.append(pivot_table)\n",
    "\n",
    "# Merging all datasets together\n",
    "#acled_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "# acled_df = acled_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad971c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "main_df = pd.read_excel(\"Data/organizedviolencecy_v23_2.xlsx\")\n",
    "# armed_conflict_df = pd.read_csv(\"Data/armed_conflict_19462022.csv\")\n",
    "\n",
    "# mil_spending_df = pd.read_csv(\"Data/mil_spending.csv\", skiprows=4)\n",
    "\n",
    "\n",
    "\n",
    "# List of country codes to keep (recognized country)\n",
    "recognized_country_codes = [\n",
    "    \"AFG\", \"ALA\", \"ALB\", \"DZA\", \"ASM\", \"AND\", \"AGO\", \"AIA\", \"ATA\", \"ATG\", \"ARG\", \"ARM\", \"ABW\", \"AUS\", \"AUT\", \"AZE\",\n",
    "    \"BHS\", \"BHR\", \"BGD\", \"BRB\", \"BLR\", \"BEL\", \"BLZ\", \"BEN\", \"BMU\", \"BTN\", \"BOL\", \"BES\", \"BIH\", \"BWA\", \"BVT\", \"BRA\",\n",
    "    \"IOT\", \"BRN\", \"BGR\", \"BFA\", \"BDI\", \"CPV\", \"KHM\", \"CMR\", \"CAN\", \"CYM\", \"CAF\", \"TCD\", \"CHL\", \"CHN\", \"CXR\", \"CCK\",\n",
    "    \"COL\", \"COM\", \"COG\", \"COD\", \"COK\", \"CRI\", \"CIV\", \"HRV\", \"CUB\", \"CUW\", \"CYP\", \"CZE\", \"DNK\", \"DJI\", \"DMA\", \"DOM\",\n",
    "    \"ECU\", \"EGY\", \"SLV\", \"GNQ\", \"ERI\", \"EST\", \"SWZ\", \"ETH\", \"FLK\", \"FRO\", \"FJI\", \"FIN\", \"FRA\", \"GUF\", \"PYF\", \"ATF\",\n",
    "    \"GAB\", \"GMB\", \"GEO\", \"DEU\", \"GHA\", \"GIB\", \"GRC\", \"GRL\", \"GRD\", \"GLP\", \"GUM\", \"GTM\", \"GGY\", \"GIN\", \"GNB\", \"GUY\",\n",
    "    \"HTI\", \"HMD\", \"VAT\", \"HND\", \"HKG\", \"HUN\", \"ISL\", \"IND\", \"IDN\", \"IRN\", \"IRQ\", \"IRL\", \"IMN\", \"ISR\", \"ITA\", \"JAM\",\n",
    "    \"JPN\", \"JEY\", \"JOR\", \"KAZ\", \"KEN\", \"KIR\", \"PRK\", \"KOR\", \"KWT\", \"KGZ\", \"LAO\", \"LVA\", \"LBN\", \"LSO\", \"LBR\", \"LBY\",\n",
    "    \"LIE\", \"LTU\", \"LUX\", \"MAC\", \"MDG\", \"MWI\", \"MYS\", \"MDV\", \"MLI\", \"MLT\", \"MHL\", \"MTQ\", \"MRT\", \"MUS\", \"MYT\", \"MEX\",\n",
    "    \"FSM\", \"MDA\", \"MCO\", \"MNG\", \"MNE\", \"MSR\", \"MAR\", \"MOZ\", \"MMR\", \"NAM\", \"NRU\", \"NPL\", \"NLD\", \"NCL\", \"NZL\", \"NIC\",\n",
    "    \"NER\", \"NGA\", \"NIU\", \"NFK\", \"MKD\", \"MNP\", \"NOR\", \"OMN\", \"PAK\", \"PLW\", \"PSE\", \"PAN\", \"PNG\", \"PRY\", \"PER\", \"PHL\",\n",
    "    \"PCN\", \"POL\", \"PRT\", \"PRI\", \"QAT\", \"REU\", \"ROU\", \"RUS\", \"RWA\", \"BLM\", \"SHN\", \"KNA\", \"LCA\", \"MAF\", \"SPM\", \"VCT\",\n",
    "    \"WSM\", \"SMR\", \"STP\", \"SAU\", \"SEN\", \"SRB\", \"SYC\", \"SLE\", \"SGP\", \"SXM\", \"SVK\", \"SVN\", \"SLB\", \"SOM\", \"ZAF\", \"SGS\",\n",
    "    \"SSD\", \"ESP\", \"LKA\", \"SDN\", \"SUR\", \"SJM\", \"SWE\", \"CHE\", \"SYR\", \"TWN\", \"TJK\", \"TZA\", \"THA\", \"TLS\", \"TGO\", \"TKL\",\n",
    "    \"TON\", \"TTO\", \"TUN\", \"TUR\", \"TKM\", \"TCA\", \"TUV\", \"UGA\", \"UKR\", \"ARE\", \"GBR\", \"USA\", \"UMI\", \"URY\", \"UZB\", \"VUT\",\n",
    "    \"VEN\", \"VNM\", \"VGB\", \"VIR\", \"WLF\", \"ESH\", \"YEM\", \"ZMB\", \"ZWE\"\n",
    "]\n",
    "# Count how many countries are in the recognized_country_codes list\n",
    "print(len(recognized_country_codes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfed3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "# Create dictionaries to map country names to ISO codes and vice versa\n",
    "country_name_to_iso = {country.name: country.alpha_3 for country in pycountry.countries}\n",
    "iso_to_country_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "df_country_name_to_iso = pd.DataFrame(list(country_name_to_iso.items()), columns=['Country Name', 'ISO_Code'])\n",
    "df_iso_to_country_name = pd.DataFrame(list(iso_to_country_name.items()), columns=['ISO_Code', 'Country Name'])\n",
    "\n",
    "# Merge the DataFrames into a single DataFrame without duplicating \"Country Name\"\n",
    "iso_df = pd.merge(df_country_name_to_iso, df_iso_to_country_name, on='ISO_Code', how='inner', suffixes=('_country', '_iso'))\n",
    "\n",
    "# Rename the \"Country Name\" column to \"Name\"\n",
    "iso_df.rename(columns={'Country Name_country': 'Name', 'ISO_Code': 'stateid'}, inplace=True)\n",
    "\n",
    "# Drop the duplicate \"Country Name_iso\" column\n",
    "iso_df.drop(columns=['Country Name_iso'], inplace=True)\n",
    "\n",
    "# Only keep the countries in the recognized_country_codes list\n",
    "iso_df = iso_df[iso_df['stateid'].isin(recognized_country_codes)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to find closest match\n",
    "def find_closest_match(country, country_list):\n",
    "    closest_match = process.extractOne(country, country_list)\n",
    "    return closest_match\n",
    "\n",
    "# Initialize an empty mapping dictionary\n",
    "mapping_dict = {}\n",
    "\n",
    "# Iterate over rows in iso_df\n",
    "for index, row in iso_df.iterrows():\n",
    "    country_name_iso = row['Name']\n",
    "    # Find the closest match in gleditsch_code DataFrame\n",
    "    closest_match = find_closest_match(country_name_iso, gleditsch_code['countryname'])\n",
    "    # Check if the score is above a certain threshold (adjust as needed)\n",
    "    if closest_match[1] >= 90:\n",
    "        # Retrieve the corresponding g_code\n",
    "        g_code = gleditsch_code.loc[gleditsch_code['countryname'] == closest_match[0], 'g_code'].iloc[0]\n",
    "        # Add the mapping to the dictionary\n",
    "        mapping_dict[country_name_iso] = g_code\n",
    "\n",
    "# Map the g_code values to iso_df based on the mapping dictionary\n",
    "iso_df['g_code'] = iso_df['Name'].map(mapping_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3ed6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stateids are not consistent for the following countries:\n",
      "['Congo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\troen\\AppData\\Local\\Temp\\ipykernel_20332\\2016274278.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  gdp_df = gdp_df.fillna(method='bfill', axis=1)\n",
      "C:\\Users\\troen\\AppData\\Local\\Temp\\ipykernel_20332\\2016274278.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  gdp_df = gdp_df.fillna(method='ffill', axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning of GDP dataset: GDP between 1989 and 2022 in current US dollars\n",
    "gdp_df = pd.read_csv(\"Data/gdp_world.csv\", skiprows=4)\n",
    "\n",
    "# Only keep Country Code is in recognized_country_codes\n",
    "gdp_df = gdp_df[gdp_df[\"Country Code\"].isin(recognized_country_codes)]\n",
    "\n",
    "# Only keep relevant columns\n",
    "gdp_df = gdp_df[[\"Country Name\", \"Country Code\", \"1989\", \"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\",\n",
    "                 \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\",\n",
    "                 \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]]\n",
    "\n",
    "# Only keep the countries recognized in the recognized_country_codes list\n",
    "gdp_df = gdp_df[gdp_df[\"Country Code\"].isin(recognized_country_codes)]\n",
    "\n",
    "# Backfill NaN values\n",
    "gdp_df = gdp_df.fillna(method='bfill', axis=1)\n",
    "\n",
    "# rename Country Code to stateid\n",
    "gdp_df = gdp_df.rename(columns={\"Country Code\": \"stateid\"})\n",
    "\n",
    "# Frontfill NaN values -> some countries miss their 2022 value\n",
    "gdp_df = gdp_df.fillna(method='ffill', axis=1)\n",
    "\n",
    "\n",
    "# Initialize the mapping dictionary\n",
    "name_mapping = {\n",
    "    \"Congo, Dem. Rep.\": \"Congo\",\n",
    "    \"Tanzania\": \"Tanzania, United Republic of\",\n",
    "    \"Turkiye\": \"Türkiye\",\n",
    "    \"Moldova\": \"Moldova, Republic of\",\n",
    "    \"Bahamas, The\": \"Bahamas\",\n",
    "    \"Macao SAR, China\": \"China\",\n",
    "    \"Korea, Rep.\": \"Korea, Republic of\",\n",
    "    \"Curacao\": \"Curaçao\",\n",
    "    \"Egypt, Arab Rep.\": \"Egypt\",\n",
    "    \"Congo, Rep.\": \"Congo\",\n",
    "    \"Bolivia\": \"Bolivia, Plurinational State of\",\n",
    "    \"Virgin Islands (U.S.)\": \"Virgin Islands, U.S.\",\n",
    "    \"St. Vincent and the Grenadines\": \"Saint Vincent and the Grenadines\",\n",
    "    \"St. Martin (French part)\": \"Saint Martin (French part)\",\n",
    "    \"Hong Kong SAR, China\": \"Hong Kong\",\n",
    "    \"British Virgin Islands\": \"Virgin Islands, British\",\n",
    "    \"Yemen, Rep.\": \"Yemen\",\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"St. Kitts and Nevis\": \"Saint Kitts and Nevis\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    # Add mappings for the countries not found in the ISO dataset\n",
    "    \"Micronesia, Fed. Sts.\": \"\",  # Change this to the correct ISO name\n",
    "    \"Lao PDR\": \"\",  # Change this to the correct ISO name\n",
    "    \"Iran, Islamic Rep.\": \"\",  # Change this to the correct ISO name\n",
    "    \"Korea, Dem. People's Rep.\": \"\",  # Change this to the correct ISO name\n",
    "    \"Kyrgyz Republic\": \"\",  # Change this to the correct ISO name\n",
    "    \"St. Lucia\": \"\",  # Change this to the correct ISO name\n",
    "    \"Slovak Republic\": \"\",  # Change this to the correct ISO name\n",
    "}\n",
    "\n",
    "# Manually update the mapping dictionary with the correct ISO names\n",
    "name_mapping[\"Micronesia, Fed. Sts.\"] = \"Micronesia, Federated States of\"\n",
    "name_mapping[\"Lao PDR\"] = \"Lao People's Democratic Republic\"\n",
    "name_mapping[\"Iran, Islamic Rep.\"] = \"Iran, Islamic Republic of\"\n",
    "name_mapping[\"Korea, Dem. People's Rep.\"] = \"Korea, Democratic People's Republic of\"\n",
    "name_mapping[\"Kyrgyz Republic\"] = \"Kyrgyzstan\"\n",
    "name_mapping[\"St. Lucia\"] = \"Saint Lucia\"\n",
    "name_mapping[\"Slovak Republic\"] = \"Slovakia\"\n",
    "\n",
    "# Apply name mapping to GDP dataset\n",
    "gdp_df['Country Name'] = gdp_df['Country Name'].map(name_mapping).fillna(gdp_df['Country Name'])\n",
    "\n",
    "gdp_countries = gdp_df['Country Name'].unique()\n",
    "iso_countries = iso_df['Name'].unique()\n",
    "\n",
    "# Check if the same stateid is used for each country in both datasets\n",
    "consistent_stateids = True\n",
    "inconsistent_countries = []\n",
    "\n",
    "for country in gdp_countries:\n",
    "    if country in iso_countries:\n",
    "        gdp_stateid = gdp_df.loc[gdp_df['Country Name'] == country, 'stateid'].iloc[0]\n",
    "        iso_stateid = iso_df.loc[iso_df['Name'] == country, 'stateid'].iloc[0]\n",
    "        \n",
    "        if gdp_stateid != iso_stateid:\n",
    "            consistent_stateids = False\n",
    "            inconsistent_countries.append(country)\n",
    "\n",
    "if consistent_stateids:\n",
    "    print(\"The same stateid is used for each country in both datasets.\")\n",
    "else:\n",
    "    print(\"The stateids are not consistent for the following countries:\")\n",
    "    print(inconsistent_countries)\n",
    "\n",
    "# No data available for North Korea; estimation from tradingeconomics.com + linear interpolation\n",
    "\n",
    "nk_gdp = {\n",
    "    1989: 15770000000,\n",
    "    1990: None,\n",
    "    1991: None,\n",
    "    1992: 11000000000,\n",
    "    1993: None,\n",
    "    1994: None,\n",
    "    1995: 4850000000,\n",
    "    1996: None,\n",
    "    1997: None,\n",
    "    1998: 11000000000,\n",
    "    1999: None,\n",
    "    2000: None,\n",
    "    2001: None,\n",
    "    2002: None,\n",
    "    2003: None,\n",
    "    2004: None,\n",
    "    2005: None,\n",
    "    2006: None,\n",
    "    2007: None,\n",
    "    2008: None,\n",
    "    2009: None,\n",
    "    2010: None,\n",
    "    2011: None,\n",
    "    2012: None,\n",
    "    2013: None,\n",
    "    2014: None,\n",
    "    2015: None,\n",
    "    2016: None,\n",
    "    2017: None,\n",
    "    2018: None,\n",
    "    2019: None,\n",
    "    2020: None,\n",
    "    2021: None,\n",
    "    2022: 20000000000\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "nk_gdp_df = pd.DataFrame.from_dict(nk_gdp, orient='index', columns=['GDP'])\n",
    "\n",
    "# Convert index to a column\n",
    "nk_gdp_df.reset_index(inplace=True)\n",
    "nk_gdp_df.rename(columns={'index': 'Year'}, inplace=True)\n",
    "\n",
    "# Interpolate missing values linearly\n",
    "nk_gdp_df['GDP'] = nk_gdp_df['GDP'].interpolate()\n",
    "\n",
    "# Provided data\n",
    "nk_gdp_values = {\n",
    "    1989: 1.577000e+10, 1990: 1.418000e+10, 1991: 1.259000e+10, 1992: 1.100000e+10, 1993: 8.950000e+09,\n",
    "    1994: 6.900000e+09, 1995: 4.850000e+09, 1996: 6.900000e+09, 1997: 8.950000e+09, 1998: 1.100000e+10,\n",
    "    1999: 1.137500e+10, 2000: 1.175000e+10, 2001: 1.212500e+10, 2002: 1.250000e+10, 2003: 1.287500e+10,\n",
    "    2004: 1.325000e+10, 2005: 1.362500e+10, 2006: 1.400000e+10, 2007: 1.437500e+10, 2008: 1.475000e+10,\n",
    "    2009: 1.512500e+10, 2010: 1.550000e+10, 2011: 1.587500e+10, 2012: 1.625000e+10, 2013: 1.662500e+10,\n",
    "    2014: 1.700000e+10, 2015: 1.737500e+10, 2016: 1.775000e+10, 2017: 1.812500e+10, 2018: 1.850000e+10,\n",
    "    2019: 1.887500e+10, 2020: 1.925000e+10, 2021: 1.962500e+10, 2022: 2.000000e+10\n",
    "}\n",
    "\n",
    "# Locate the row with stateid equal to \"PRK\" in gdp_df\n",
    "prk_row_index = gdp_df.index[gdp_df['stateid'] == 'PRK'].tolist()[0]\n",
    "\n",
    "# Replace the values in columns 1989 to 2022 with the provided values\n",
    "for year, gdp_value in nk_gdp_values.items():\n",
    "    gdp_df.at[prk_row_index, str(year)] = gdp_value\n",
    "\n",
    "\n",
    "# No data for Virgin Islands and Gibraltar: drop the columns (not used in main_df)\n",
    "gdp_df = gdp_df[~gdp_df['stateid'].isin(['VGB', 'GIB'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02b51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the main_df\n",
    "\n",
    "# Remove columns with disagregated deaths. Only keep \"prior\" information and cumulative total deaths.\n",
    "main_df = main_df.loc[:, ['country_id_cy', 'country_cy', 'year_cy', 'region_cy', 'sb_dyad_count_cy', 'sb_dyad_ids_cy',\n",
    "                          'sb_intrastate_dyad_count_cy', 'sb_intrastate_main_govt_inv_incomp_cy',\n",
    "                          'sb_interstate_dyad_count_cy',\n",
    "                          'sb_interstate_main_govt_inv_incomp_cy', 'ns_dyad_count_cy', 'ns_dyad_ids_cy',\n",
    "                          'os_dyad_count_cy', 'os_dyad_ids_cy', 'os_main_govt_inv_cy', 'os_any_govt_inv_cy',\n",
    "                          'os_nsgroup_inv_cy', 'cumulative_total_deaths_parties_in_orgvio_cy',\n",
    "                          'cumulative_total_deaths_civilians_in_orgvio_cy', 'cumulative_total_deaths_unknown_in_orgvio_cy',\n",
    "                          'cumulative_total_deaths_in_orgvio_best_cy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586a6399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incompatibility\n",
      "0    5161\n",
      "2     896\n",
      "1     474\n",
      "3      13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the main_df\n",
    "\n",
    "# Importing battle_death data set, to get the dyads \n",
    "\n",
    "# importing battle_death dataset\n",
    "battle_death_df = pd.read_csv(\"Data/battle_deaths.csv\")\n",
    "\n",
    "\n",
    "# Function to get incompatibility value from battle_death_df based on dyad_id\n",
    "def get_incompatibility(dyad_ids):\n",
    "    # Initialize a list to store incompatibility values for each dyad_id\n",
    "    incompatibilities = []\n",
    "    \n",
    "    # Split the dyad_ids separated by \";\"\n",
    "    dyad_ids = dyad_ids.split(\";\")\n",
    "    \n",
    "    # Iterate over each dyad_id\n",
    "    for dyad_id in dyad_ids:\n",
    "        # Check if dyad_id is 'NO_DYAD'\n",
    "        if dyad_id != 'NO_DYAD':\n",
    "            # Look up the incompatibility value from battle_death_df\n",
    "            incompatibility = battle_death_df.loc[battle_death_df['dyad_id'] == int(dyad_id), 'incompatibility'].values\n",
    "            # If the dyad_id is found, append the incompatibility value to the list\n",
    "            if len(incompatibility) > 0:\n",
    "                incompatibilities.extend(incompatibility)\n",
    "    \n",
    "    # Return the maximum incompatibility value found for the dyad_ids\n",
    "    return max(incompatibilities) if incompatibilities else None\n",
    "\n",
    "# Create a new column 'incompatibility' in main_df using the function\n",
    "main_df['incompatibility'] = main_df['sb_dyad_ids_cy'].apply(get_incompatibility)\n",
    "\n",
    "main_df.insert(main_df.columns.get_loc('sb_dyad_ids_cy') + 1, 'incompatibility', main_df.pop('incompatibility'))\n",
    "\n",
    "# Replace NaN values from 'incompatibility' column with 0 (= no state incompatibility)\n",
    "main_df['incompatibility'] = main_df['incompatibility'].fillna(0)\n",
    "\n",
    "# Change Incompatibility values to integers\n",
    "main_df['incompatibility'] = main_df['incompatibility'].astype(int)\n",
    "\n",
    "# Count each occurrence of incompatibility values in main_df\n",
    "print(main_df['incompatibility'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c861d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_state = pd.read_csv(\"Data/Nonstate_v23_1.csv\")\n",
    "\n",
    "# Convert 'dyad_id' column in 'non_state' DataFrame to string type\n",
    "non_state['dyad_id'] = non_state['dyad_id'].astype(str)\n",
    "\n",
    "# Explode the 'ns_dyad_ids_cy' column to create multiple rows for each id\n",
    "exploded_df = main_df.explode('ns_dyad_ids_cy')\n",
    "\n",
    "# Merge with 'non_state' DataFrame on 'dyad_id'\n",
    "merged_df = exploded_df.merge(non_state, how='left', left_on='ns_dyad_ids_cy', right_on='dyad_id')\n",
    "\n",
    "# Convert 'org' column to string type\n",
    "merged_df['org'] = merged_df['org'].astype(str)\n",
    "\n",
    "# Group by the original index and aggregate the 'org' values\n",
    "grouped_df = merged_df.groupby(merged_df.index)['org'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "\n",
    "# Merge the aggregated 'org' values back to 'main_df'\n",
    "main_df = main_df.merge(grouped_df, how='left', left_index=True, right_on='index')\n",
    "\n",
    "# Drop unnecessary columns except 'ns_dyad_ids_cy'\n",
    "main_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Rename the merged column to 'org'\n",
    "main_df.rename(columns={'org_x': 'org'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5283451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"nan\" with 0 in 'org' column\n",
    "main_df['org'] = main_df['org'].replace('nan', 0)\n",
    "\n",
    "# Drop columns: sb_instrastate_dyad_count, sb_interstate_dyad_count\n",
    "main_df = main_df.drop(columns=['sb_intrastate_dyad_count_cy', 'sb_interstate_dyad_count_cy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1725cdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sb_dyad_count_cy\n",
      "0     5161\n",
      "1      651\n",
      "2      305\n",
      "3      182\n",
      "4      102\n",
      "5       48\n",
      "6       28\n",
      "7       21\n",
      "8        8\n",
      "9        7\n",
      "10       7\n",
      "11       7\n",
      "14       6\n",
      "12       4\n",
      "15       3\n",
      "13       3\n",
      "16       1\n",
      "Name: count, dtype: int64\n",
      "ns_dyad_count_cy\n",
      "0     5773\n",
      "1      342\n",
      "2      144\n",
      "3       73\n",
      "4       45\n",
      "5       34\n",
      "6       30\n",
      "7       17\n",
      "8       16\n",
      "9       14\n",
      "11      12\n",
      "10       7\n",
      "12       5\n",
      "15       5\n",
      "24       4\n",
      "21       3\n",
      "13       3\n",
      "17       3\n",
      "23       2\n",
      "14       2\n",
      "20       2\n",
      "16       2\n",
      "19       2\n",
      "22       1\n",
      "26       1\n",
      "18       1\n",
      "35       1\n",
      "Name: count, dtype: int64\n",
      "os_dyad_count_cy\n",
      "0     5162\n",
      "1      736\n",
      "2      317\n",
      "3      132\n",
      "4       93\n",
      "5       44\n",
      "6       15\n",
      "8       13\n",
      "10       6\n",
      "11       5\n",
      "9        5\n",
      "7        4\n",
      "12       4\n",
      "16       4\n",
      "15       1\n",
      "23       1\n",
      "24       1\n",
      "13       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of occurences of \"0\" in sb_dyad_count_cy, in ns_dyad_count_cy and in os_dyad_count_cy \n",
    "print(main_df['sb_dyad_count_cy'].value_counts())\n",
    "print(main_df['ns_dyad_count_cy'].value_counts())\n",
    "print(main_df['os_dyad_count_cy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbce02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5dd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23697b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterate over each country\n",
    "for country_name, country_data in education_df.groupby('Country Name'):\n",
    "    # Plot the education level over the years\n",
    "    plt.plot(education_df.columns[4:], country_data.iloc[:, 4:].values.flatten(), label=country_name)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Education Level Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Education Level (%)')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Move legend outside the graph\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying something for the missing data\n",
    "for country_name, country_data in education_df.groupby('Country Name'):\n",
    "    # Interpolate missing values using linear interpolation\n",
    "    education_df.loc[education_df['Country Name'] == country_name, '1989':'2023'] = country_data.loc[:, '1989':'2023'].interpolate(method='linear', axis=1)\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterate over each country\n",
    "for country_name, country_data in education_df.groupby('Country Name'):\n",
    "    # Plot the education level over the years\n",
    "    plt.plot(education_df.columns[4:], country_data.iloc[:, 4:].values.flatten(), label=country_name)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Education Level Over the Years (Filled Data)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Education Level (%)')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Move legend outside the graph\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1558c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8e0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
